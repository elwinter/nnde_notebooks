{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5169f6-efe8-487c-88a1-9f20541282d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard Python modules.\n",
    "import datetime\n",
    "import importlib\n",
    "import os\n",
    "import platform\n",
    "import sys\n",
    "\n",
    "# Import 3rd-party modules.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow.\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35241708-a05f-4642-8bf2-793fb5c6a811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 64-bit math in TensorFlow.\n",
    "tf.keras.backend.set_floatx(\"float64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaefed45-1e15-4758-b8da-1df2d44f29b0",
   "metadata": {},
   "source": [
    "# Case 5: Compressible, adiabatic, $\\frac {dP} {dx}$ constant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab23662-6d3d-4e51-8538-aec53722959a",
   "metadata": {},
   "source": [
    "Consider one-dimensional, adiabatic, compressible fluid flow in a channel. For simplicity, assume the channel is a pipe with a circular cross-section of diameter $D$. Flow in the channel is controlled by the Bernoulli equation:\n",
    "\n",
    "\\begin{equation}\n",
    "    P + \\frac {1} {2} \\rho u^2 + \\rho g h = constant = C_1\n",
    "\\end{equation}\n",
    "\n",
    "where $P$ is the pressure, $\\rho$ is the mass density, $u$ is the fluid flow speed, $g$ is the acceleration due to gravity, and $h$ is the height of the fluid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d5d1c1-cc6b-4c9c-8eea-88e2f8a6ba30",
   "metadata": {},
   "source": [
    "Neglecting gravity, this equation can be rearranged to solve for the flow speed $u$ as a function of the pressure $P$:\n",
    "\n",
    "\\begin{equation}\n",
    "    u = \\left[ \\frac {2} {\\rho} \\left( C_1 - P \\right) \\right]^{\\frac {1} {2}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fe365c-6f86-4eec-bdda-2eea92f92dbb",
   "metadata": {},
   "source": [
    "The pressure and density are related by the adiabatic relation:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac {P} {\\rho^{\\gamma}} = constant = C_2\n",
    "\\end{equation}\n",
    "\n",
    "where $\\gamma$ is the adiabtic index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2c50a7-af15-4080-9b55-285ca016a918",
   "metadata": {},
   "source": [
    "Rewriting the speed equation with this relation gives:\n",
    "\n",
    "\\begin{equation}\n",
    "    u = \\left[ 2 \\left( \\frac {C_2} {P} \\right)^{\\frac {1} {\\gamma}} \\left( C_1 - P \\right) \\right]^{\\frac {1} {2}} \\\\\n",
    "    = 2 C_2^{\\frac {1} {2 \\gamma}} \\left( C_1 P^{- \\frac {1} {\\gamma}} - P^{1 - \\frac {1} {\\gamma}} \\right)^{\\frac {1} {2}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d14c60c-08af-43b8-88da-696a15a03382",
   "metadata": {},
   "source": [
    "The change in speed is computed using the chain rule:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac {du} {dx} = \\frac {du} {dP} \\frac {dP} {dx} \\\\\n",
    "    = 2 C_2^{\\frac {1} {2 \\gamma}} \\frac {1} {2} \\left( C_1 P^{- \\frac{1} {\\gamma}} - P^{1 - \\frac {1} {\\gamma}} \\right)^{- \\frac {1} {2}} \\left[ - \\frac{C_1} {\\gamma} P^{-1 - \\frac {1} {\\gamma}} - \\left( 1 - \\frac {1} {\\gamma} \\right) P^{- \\frac {1} {\\gamma}} \\right] \\frac {dP} {dx} \\\\\n",
    "    = 2 C_2^{\\frac {1} {2 \\gamma}} \\left[ - \\frac {C_1} {\\gamma} P^{-1 - \\frac {1} {\\gamma}} - \\left( 1 - \\frac {1} {\\gamma} \\right) P^{- \\frac {1} {\\gamma}} \\right]^{\\frac {1} {2}} \\frac {dP} {dx}\n",
    "\\end{equation}\n",
    "\n",
    "where $x$ is the linear distance along the channel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0878b7e8-7b4b-4bd6-a0ef-39f3fcbd48d1",
   "metadata": {},
   "source": [
    "Now assume the existence of a constant pressure gradient $dP/dx$:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac {dP} {dx} = C_3 \\Rightarrow P(x) = P_0 + C_3 x\n",
    "\\end{equation}\n",
    "\n",
    "where $P_0$ is $P(x = 0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592b90c6-0815-4518-a4c0-775920c69c8a",
   "metadata": {},
   "source": [
    "Inserting the pressure gradient into the equation for the speed gradient, we get:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac {du} {dx} = \\left( 2 C_2^{1/\\gamma} \\right)^{1/2} \\left( - \\frac {C_1} {\\gamma} P^{-1 - 1/\\gamma} - \\left( 1 - \\frac {1} {\\gamma} \\right) P^{-1/\\gamma} \\right)^{1/2} \\frac {dP} {dx}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2036e092-60ea-42ca-82a1-b56c0e3d5859",
   "metadata": {},
   "source": [
    "Now assume $\\rho_0 = 1$, $P_0 = 1$, $u_0 = 1$, and $C_2 = -1$, and select $m$ and $T$ such that $C_3 = 1$. The constant $C_1 = \\frac {3} {2}$. The differential equation becomes:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac {du} {dx} = XXX\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f835c6ef-8754-4654-bb12-27935df65d38",
   "metadata": {},
   "source": [
    "This ordinary differential equation has the analytical solution $u_a(x)$:\n",
    "\n",
    "\\begin{equation}\n",
    "    u_a(x) = XXX\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93665a34-95b5-4367-97b6-f3dbf7af333a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_name = \"case5\"\n",
    "u_analytical = lambda x: (3*(1 - x)**(-3/5) - 2*(1 - x)**(2/5))**(1/2)\n",
    "du_dx_analytical = lambda x: (3*(1 - x)**(-3/5) - 2*(1 - x)**(2/5))**(-1/2)*(9/10*(1 - x)**(-8/5) + 2/5*(1 - x)**(-3/5))\n",
    "\n",
    "nx = 101\n",
    "xa = np.linspace(0, 0.9, nx)  # Singularity at x = 1.\n",
    "ua = u_analytical(xa)\n",
    "dua_dx = du_dx_analytical(xa)\n",
    "\n",
    "# Plot the analytical solution and derivative.\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(xa, ua, label=\"$u_a$\")\n",
    "ax.plot(xa, dua_dx, label=\"$du_a/dx$\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"$u_a$ or $du_a/dx$\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "ax.set_title(\"Analytical solution and derivative for %s\" % eq_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f775fe50-2915-45a7-be0f-9626078ea930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_system_information():\n",
    "    print(\"System report:\")\n",
    "    print(datetime.datetime.now())\n",
    "    print(\"Host name: %s\" % platform.node())\n",
    "    print(\"OS: %s\" % platform.platform())\n",
    "    print(\"uname:\", platform.uname())\n",
    "    print(\"Python version: %s\" % sys.version)\n",
    "    print(\"Python build:\", platform.python_build())\n",
    "    print(\"Python compiler: %s\" % platform.python_compiler())\n",
    "    print(\"Python implementation: %s\" % platform.python_implementation())\n",
    "    # print(\"Python file: %s\" % __file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5108993-c34b-4665-8b34-eca840a2710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_output_directory(path=None):\n",
    "    path_noext, ext = os.path.splitext(path)\n",
    "    output_dir = path_noext\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "    return output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a7d07f-be4d-4c51-8fcb-d9ad048c88cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnde.math.trainingdata import create_training_grid2\n",
    "\n",
    "def create_training_data(*n_train):\n",
    "    x_train = np.linspace(0, 0.9, n_train[0])\n",
    "    return x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7a172e-2e3b-436e-a8c2-30ec3a39686b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(H, w0_range, u0_range, v0_range):\n",
    "    hidden_layer_1 = tf.keras.layers.Dense(\n",
    "        units=H, use_bias=True,\n",
    "        activation=tf.keras.activations.sigmoid,\n",
    "        kernel_initializer=tf.keras.initializers.RandomUniform(*w0_range),\n",
    "        bias_initializer=tf.keras.initializers.RandomUniform(*u0_range)\n",
    "    )\n",
    "    output_layer = tf.keras.layers.Dense(\n",
    "        units=1,\n",
    "        activation=tf.keras.activations.linear,\n",
    "        kernel_initializer=tf.keras.initializers.RandomUniform(*v0_range),\n",
    "        use_bias=False,\n",
    "    )\n",
    "    model = tf.keras.Sequential([hidden_layer_1, output_layer])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c85ace-8b94-4371-a963-172448629e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_system_information()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e603488d-9f36-4c11-9788-4b13bcf1b83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the output directory.\n",
    "path = os.path.join(\".\", eq_name)\n",
    "output_dir = create_output_directory(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220421d2-169f-48ae-a3c7-ec5bc4e2ac6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters.\n",
    "\n",
    "# Training optimizer\n",
    "optimizer_name = \"Adam\"\n",
    "\n",
    "# Initial parameter ranges\n",
    "w0_range = [-0.1, 0.1]\n",
    "u0_range = [-0.1, 0.1]\n",
    "v0_range = [-0.1, 0.1]\n",
    "\n",
    "# Maximum number of training epochs.\n",
    "max_epochs = 40000\n",
    "\n",
    "# Learning rate.\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Absolute tolerance for consecutive loss function values to indicate convergence.\n",
    "tol = 1e-6\n",
    "\n",
    "# Number of hidden nodes.\n",
    "H = 10\n",
    "\n",
    "# Number of dimensions\n",
    "m = 1\n",
    "\n",
    "# Number of training points in each dimension.\n",
    "nx_train = 11\n",
    "n_train = nx_train\n",
    "\n",
    "# Random number generator seed.\n",
    "random_seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa423e4b-b35c-4e83-ab82-412d8d83d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save the training data.\n",
    "x_train = create_training_data(nx_train)\n",
    "np.savetxt(os.path.join(output_dir,'x_train.dat'), x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8621e73c-c912-470b-a22a-808b28007903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the differential equations using TensorFlow operations.\n",
    "\n",
    "@tf.function\n",
    "def ode_u(x, u, du_dx):\n",
    "    G = du_dx - (3*(1 - x)**(-3/5) - 2*(1 - x)**(2/5))**(-1/2)*(9/10*(1 - x)**(-8/5) + 2/5*(1 - x)**(-3/5))\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4979edb5-19bc-4e1e-87e9-4691e3ac1c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the trial functions.\n",
    "\n",
    "@tf.function\n",
    "def Y_trial_u(x, N):\n",
    "    A = tf.constant([[1.0]], dtype=\"float64\")\n",
    "    P = x\n",
    "    Y = A + P*N\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dd6c43-e136-4755-a99f-ad0357f2e444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the models.\n",
    "model_u = build_model(H, w0_range, u0_range, v0_range)\n",
    "\n",
    "# Create the optimizer.\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Create history variables.\n",
    "losses_u = []\n",
    "losses = []\n",
    "phist_u = []\n",
    "\n",
    "# Set the random number seed for reproducibility.\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "# Rename the training data Variable for convenience, just for training.\n",
    "# shape (n_train, m)\n",
    "x_train_var = tf.Variable(np.array(x_train).reshape((n_train, m)), name=\"x_train\")\n",
    "x = x_train_var\n",
    "\n",
    "# Clear the convergence flag to start.\n",
    "converged = False\n",
    "\n",
    "# Train the model.\n",
    "\n",
    "print(\"Hyperparameters: n_train = %s, H = %s, max_epochs = %s, optimizer = %s, learning_rate = %s\"\n",
    "      % (n_train, H, max_epochs, optimizer_name, learning_rate))\n",
    "t_start = datetime.datetime.now()\n",
    "print(\"Training started at\", t_start)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "\n",
    "    # Run the forward pass.\n",
    "    with tf.GradientTape(persistent=True) as tape1:\n",
    "        with tf.GradientTape(persistent=True) as tape0:\n",
    "\n",
    "            # Compute the network outputs at the training points.\n",
    "            N_u = model_u(x)\n",
    "\n",
    "            # Compute the trial solutions.\n",
    "            u = Y_trial_u(x, N_u)\n",
    "\n",
    "        # Compute the gradients of the trial solutions wrt inputs.\n",
    "        du_dx = tape0.gradient(u, x)\n",
    "\n",
    "        # Compute the estimates of the differential equations.\n",
    "        G_u = ode_u(x, u, du_dx)\n",
    "\n",
    "        # Compute the loss functions.\n",
    "        L_u = tf.math.sqrt(tf.reduce_sum(G_u**2)/n_train)\n",
    "        L = L_u\n",
    "\n",
    "    # Save the current losses.\n",
    "    losses_u.append(L_u)\n",
    "    losses.append(L.numpy())\n",
    "\n",
    "    # Check for convergence.\n",
    "    if epoch > 0:\n",
    "        loss_delta = losses[-1] - losses[-2]\n",
    "        if abs(loss_delta) <= tol:\n",
    "            converged = True\n",
    "            break\n",
    "\n",
    "    # Compute the gradient of the loss function wrt the network parameters.\n",
    "    pgrad_u = tape1.gradient(L, model_u.trainable_variables)\n",
    "\n",
    "    # Save the parameters used in this epoch.\n",
    "    phist_u.append(\n",
    "        np.hstack(\n",
    "            (model_u.trainable_variables[0].numpy().reshape((m*H,)),    # w (m, H) matrix -> (m*H,) row vector\n",
    "             model_u.trainable_variables[1].numpy(),       # u (H,) row vector\n",
    "             model_u.trainable_variables[2][:, 0].numpy()) # v (H, 1) column vector\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Update the parameters for this epoch.\n",
    "    optimizer.apply_gradients(zip(pgrad_u, model_u.trainable_variables))\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Ending epoch %s, loss function = %f\" % (epoch, L.numpy()))\n",
    "\n",
    "# Save the parameters used in the last epoch.\n",
    "phist_u.append(\n",
    "    np.hstack(\n",
    "        (model_u.trainable_variables[0].numpy().reshape((m*H,)),    # w (m, H) matrix -> (m*H,) row vector\n",
    "         model_u.trainable_variables[1].numpy(),       # u (H,) row vector\n",
    "         model_u.trainable_variables[2][:, 0].numpy()) # v (H, 1) column vector\n",
    "    )\n",
    ")\n",
    "\n",
    "n_epochs = epoch + 1\n",
    "\n",
    "t_stop = datetime.datetime.now()\n",
    "print(\"Training stopped at\", t_stop)\n",
    "t_elapsed = t_stop - t_start\n",
    "print(\"Total training time was %s seconds.\" % t_elapsed.total_seconds())\n",
    "print(\"Epochs: %d\" % n_epochs)\n",
    "print(\"Final value of loss function: %f\" % losses[-1])\n",
    "print(\"converged = %s\" % converged)\n",
    "\n",
    "# Save the parameter histories.\n",
    "np.savetxt(os.path.join(output_dir, 'phist_u.dat'), np.array(phist_u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c58b2f-0cfa-458d-b52c-68cd600404c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and save the trained results at training points.\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    N_u = model_u(x)\n",
    "    ut_train = Y_trial_u(x, N_u)\n",
    "dut_dx_train = tape.gradient(ut_train, x)\n",
    "np.savetxt(os.path.join(output_dir, 'ut_train.dat'), ut_train.numpy().reshape((n_train,)))\n",
    "np.savetxt(os.path.join(output_dir, 'dut_dx_train.dat'), dut_dx_train.numpy())\n",
    "\n",
    "# Compute and save the analytical solution and derivative at training points.\n",
    "ua_train = np.array([u_anal(x) for x in x_train])\n",
    "dua_dx_train = np.array([du_dx_anal(x) for x in x_train])\n",
    "np.savetxt(os.path.join(output_dir,'ua_train.dat'), ua_train)\n",
    "np.savetxt(os.path.join(output_dir,'dua_dx_train.dat'), dua_dx_train)\n",
    "\n",
    "# Compute and save the error in the trained solution and derivative at training points.\n",
    "ut_err_train = ut_train.numpy().reshape((nx_train)) - ua_train\n",
    "dut_dx_err_train = dut_dx_train.numpy().reshape((nx_train)) - dua_dx_train\n",
    "np.savetxt(os.path.join(output_dir, 'ut_err_train.dat'), ut_err_train)\n",
    "np.savetxt(os.path.join(output_dir, 'dut_dx_err_train.dat'), dut_dx_err_train)\n",
    "\n",
    "# Compute the final RMS errors in the solution at the training points.\n",
    "ut_rmse_train = np.sqrt(np.sum(ut_err_train**2)/n_train)\n",
    "print(\"ut_rmse_train = %s\" % ut_rmse_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ea1aba-cb1f-4de5-ad99-ca28fc447258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss function history.\n",
    "plt.semilogy(losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss function ($\\sqrt {\\sum G_i^2/N}$)\")\n",
    "plt.grid()\n",
    "plt.title(\"Loss function evolution for %s\\n%s optimizer, $\\eta$=%s, H=%s, $n_{train}$=%s, epochs=%s\" %\n",
    "          (eq_name, optimizer_name, learning_rate, H, n_train, n_epochs))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5fc2c4-1298-4458-8b0a-68ce67d325c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the the trained solutions and derivatives at the training points.\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(x_train, ut_train, label=\"$u_{trained}$\")\n",
    "plt.plot(x_train, dut_dx_train, label=\"$du_{trained}/dx$\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Trained values\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.title(\"Trained solution and derivatives at training points for %s\\n%s optimizer, $\\eta=%s$, $H=%s$, $n_{train}=%s$, epochs=$%s$\" %\n",
    "          (eq_name, optimizer_name, learning_rate, H, n_train, n_epochs))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733786a2-167b-45d2-bee6-0c3668108cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the errors in the trained solutions and derivatives at the training points.\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(x_train, ut_err_train, label=\"$u_{trained}$\")\n",
    "plt.plot(x_train, dut_dx_err_train, label=\"$du_{trained}/dx$\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Absolure error in trained values\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.title(\"Trained solution and derivatives at training points for %s\\n%s optimizer, $\\eta=%s$, $H=%s$, $n_{train}=%s$, epochs=$%s$\" %\n",
    "          (eq_name, optimizer_name, learning_rate, H, n_train, n_epochs))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de673c91-49d1-4ea6-ab83-4e40d31b2cb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
