{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5169f6-efe8-487c-88a1-9f20541282d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 2.7.16 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/usr/bin/python -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Import standard Python modules.\n",
    "import datetime\n",
    "import os\n",
    "import platform\n",
    "import sys\n",
    "\n",
    "# Import 3rd-party modules.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Import TensorFlow.\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35241708-a05f-4642-8bf2-793fb5c6a811",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use 64-bit math in TensorFlow.\n",
    "tf.keras.backend.set_floatx(\"float64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02d70a3-a144-44f1-a69c-97c3742d0fc5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Case 1: Incompressible, $\\frac {dP} {dx}$ constant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723d6bea-94e6-42b5-9870-754178fc015a",
   "metadata": {},
   "source": [
    "Consider one-dimensional, incompressible fluid flow in a channel. For simplicity, assume the channel is a pipe with a circular cross-section of diameter $D$. Flow in the channel is controlled by the Bernoulli equation:\n",
    "\n",
    "\\begin{equation}\n",
    "    P + \\frac {1} {2} \\rho u^2 + \\rho g h = constant = C_1\n",
    "\\end{equation}\n",
    "\n",
    "where $P$ is the pressure, $\\rho$ is the mass density, $u$ is the fluid flow speed, $g$ is the acceleration due to gravity, and $h$ is the height of the fluid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c167c9-da75-4775-8f44-8f2d3e75bcfb",
   "metadata": {},
   "source": [
    "Neglecting gravity, this equation can be rearranged to solve for the flow speed $u$ as a function of the pressure $P$:\n",
    "\n",
    "\\begin{equation}\n",
    "    u = \\left[ \\frac {2} {\\rho} \\left( C_1 - P \\right) \\right]^{\\frac {1} {2}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6319284-50a6-466e-9dad-bc8f107cd875",
   "metadata": {},
   "source": [
    "Since the fluid is incompressible the density $\\rho$ is constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4bc676-a557-4215-9a1b-9c5f0f1dec18",
   "metadata": {},
   "source": [
    "The speed is therefore a function only of the pressure, as shown in the equation above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5633e4-95b7-4ab1-83ed-83a523e0b780",
   "metadata": {},
   "source": [
    "The change in speed is computed using the chain rule:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac {du} {dx} = \\frac {du} {dP} \\frac {dP} {dx} \\\\\n",
    "    = \\frac {1} {2} \\left[ \\frac {2} {\\rho} \\left( C_1 - P \\right) \\right]^{- \\frac {1} {2}} \\left( - \\frac {2} {\\rho} \\frac {dP} {dx} \\right) \\\\\n",
    "    = - \\left( \\frac {1} {2 \\rho} \\right)^{ \\frac {1} {2}} \\left( C_1 - P \\right)^{- \\frac {1} {2}} \\frac {dP} {dx}\n",
    "\\end{equation}\n",
    "\n",
    "where $x$ is the linear distance along the channel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3c9f55-be9f-4f21-89fc-08f7562e5a9b",
   "metadata": {},
   "source": [
    "Now assume the existence of a constant pressure gradient $\\frac {dP} {dx}$:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac {dP} {dx} = C_2 \\Rightarrow P(x) = P_0 + C_2 x\n",
    "\\end{equation}\n",
    "\n",
    "where $P_0$ is $P(x = 0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcefa03-22ab-4a65-a1cc-0bb8757afd23",
   "metadata": {},
   "source": [
    "Inserting the pressure gradient into the equation for the speed gradient, we get:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac {du} {dx} = - \\frac {C_2} { \\left( 2 \\rho \\right)^{\\frac {1} {2}}} \\left( C_1 - P_0 - C_2 x \\right)^{- \\frac {1} {2}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca05c2f8-080c-45cb-95e0-d7b02f7b04f8",
   "metadata": {},
   "source": [
    "Now assume $\\rho = 1$, $P_0 = 1$, $u_0 = 1$, and $C_2 = -1$. The constant $C_1 = \\frac {3} {2}$, and the differential equation becomes:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac {du} {dx} = \\frac {1} {\\sqrt {2}} \\left( x + \\frac {1} {2} \\right)^{- \\frac {1} {2}} \\\\\n",
    "    = \\left( 2 x + 1 \\right)^{- \\frac {1} {2}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524ffdf6-c6ba-421b-9091-bb2fb8997001",
   "metadata": {},
   "source": [
    "This ordinary differential equation has the analytical solution $u_a(x)$:\n",
    "\n",
    "\\begin{equation}\n",
    "    u_a(x) = \\sqrt {2} \\left( x + \\frac {1} {2} \\right)^{\\frac {1} {2}} \\\\\n",
    "    = (2 x + 1)^{\\frac {1} {2}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd5f7f1-0d37-4c19-88bf-f74b6d36aa8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eq_name = \"case1\"\n",
    "\n",
    "# Define the analytical solution and derivative.\n",
    "u_analytical = lambda x: (2*x + 1)**0.5\n",
    "du_dx_analytical = lambda x: (2*x + 1)**-0.5\n",
    "\n",
    "# Compute the analytical solution and derivative.\n",
    "nx = 101\n",
    "xa = np.linspace(0, 1, nx)\n",
    "ua = u_analytical(xa)\n",
    "dua_dx = du_dx_analytical(xa)\n",
    "\n",
    "# Plot the analytical solution and derivative.\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(xa, ua, label=\"$u_a$\")\n",
    "ax.plot(xa, dua_dx, label=\"$du_a/dx$\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"$u_a$ or $du_a/dx$\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "ax.set_title(\"Analytical solution and derivative for %s\" % eq_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1268a8b4-1ffe-4f2e-a12c-756736e01c90",
   "metadata": {},
   "source": [
    "# Solving the equation with a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f775fe50-2915-45a7-be0f-9626078ea930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_system_information():\n",
    "    print(\"System report:\")\n",
    "    print(datetime.datetime.now())\n",
    "    print(\"Host name: %s\" % platform.node())\n",
    "    print(\"OS: %s\" % platform.platform())\n",
    "    print(\"uname:\", platform.uname())\n",
    "    print(\"Python version: %s\" % sys.version)\n",
    "    print(\"Python build:\", platform.python_build())\n",
    "    print(\"Python compiler: %s\" % platform.python_compiler())\n",
    "    print(\"Python implementation: %s\" % platform.python_implementation())\n",
    "    # print(\"Python file: %s\" % __file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5108993-c34b-4665-8b34-eca840a2710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_output_directory(path=None):\n",
    "    path_noext, ext = os.path.splitext(path)\n",
    "    output_dir = path_noext\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "    return output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a7d07f-be4d-4c51-8fcb-d9ad048c88cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnde.math.trainingdata import create_training_grid2\n",
    "\n",
    "def create_training_data(*n_train):\n",
    "    x_train = np.linspace(0, 1, n_train[0])\n",
    "    return x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7a172e-2e3b-436e-a8c2-30ec3a39686b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(H, w0_range, u0_range, v0_range):\n",
    "    hidden_layer_1 = tf.keras.layers.Dense(\n",
    "        units=H, use_bias=True,\n",
    "        activation=tf.keras.activations.sigmoid,\n",
    "        kernel_initializer=tf.keras.initializers.RandomUniform(*w0_range),\n",
    "        bias_initializer=tf.keras.initializers.RandomUniform(*u0_range)\n",
    "    )\n",
    "    output_layer = tf.keras.layers.Dense(\n",
    "        units=1,\n",
    "        activation=tf.keras.activations.linear,\n",
    "        kernel_initializer=tf.keras.initializers.RandomUniform(*v0_range),\n",
    "        use_bias=False,\n",
    "    )\n",
    "    model = tf.keras.Sequential([hidden_layer_1, output_layer])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c85ace-8b94-4371-a963-172448629e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_system_information()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e603488d-9f36-4c11-9788-4b13bcf1b83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the output directory.\n",
    "path = os.path.join(\".\", eq_name)\n",
    "output_dir = create_output_directory(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220421d2-169f-48ae-a3c7-ec5bc4e2ac6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters.\n",
    "\n",
    "# Training optimizer\n",
    "optimizer_name = \"Adam\"\n",
    "\n",
    "# Initial parameter ranges\n",
    "w0_range = [-0.1, 0.1]\n",
    "u0_range = [-0.1, 0.1]\n",
    "v0_range = [-0.1, 0.1]\n",
    "\n",
    "# Maximum number of training epochs.\n",
    "max_epochs = 40000\n",
    "\n",
    "# Learning rate.\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Absolute tolerance for consecutive loss function values to indicate convergence.\n",
    "tol = 1e-6\n",
    "\n",
    "# Number of hidden nodes.\n",
    "H = 10\n",
    "\n",
    "# Number of dimensions\n",
    "m = 1\n",
    "\n",
    "# Number of training points in each dimension.\n",
    "nx_train = 11\n",
    "n_train = nx_train\n",
    "\n",
    "# Random number generator seed.\n",
    "random_seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa423e4b-b35c-4e83-ab82-412d8d83d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save the training data.\n",
    "x_train = create_training_data(nx_train)\n",
    "np.savetxt(os.path.join(output_dir, \"x_train.dat\"), x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d242de8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8621e73c-c912-470b-a22a-808b28007903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the differential equation using TensorFlow operations.\n",
    "\n",
    "@tf.function\n",
    "def ode_u(x, u, du_dx):\n",
    "    G = du_dx - (2*x + 1)**-0.5\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4979edb5-19bc-4e1e-87e9-4691e3ac1c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the trial function using TensorFlow operations.\n",
    "\n",
    "@tf.function\n",
    "def Y_trial_u(x, N):\n",
    "    A = tf.constant([[1.0]], dtype=\"float64\")\n",
    "    P = x\n",
    "    Y = A + P*N\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dd6c43-e136-4755-a99f-ad0357f2e444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model.\n",
    "model_u = build_model(H, w0_range, u0_range, v0_range)\n",
    "\n",
    "# Create the optimizer.\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Create history variables.\n",
    "losses_u = []\n",
    "losses = []\n",
    "phist_u = []\n",
    "\n",
    "# Set the random number seed for reproducibility.\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "# Rename the training data Variable for convenience, just for training.\n",
    "# shape (n_train, m)\n",
    "x_train_var = tf.Variable(np.array(x_train).reshape((n_train, m)), name=\"x_train\")\n",
    "x = x_train_var\n",
    "\n",
    "# Clear the convergence flag to start.\n",
    "converged = False\n",
    "\n",
    "# Train the model.\n",
    "\n",
    "print(\"Hyperparameters: n_train = %s, H = %s, max_epochs = %s, optimizer = %s, learning_rate = %s\"\n",
    "      % (n_train, H, max_epochs, optimizer_name, learning_rate))\n",
    "t_start = datetime.datetime.now()\n",
    "print(\"Training started at\", t_start)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "\n",
    "    # Run the forward pass.\n",
    "    with tf.GradientTape(persistent=True) as tape1:\n",
    "        with tf.GradientTape(persistent=True) as tape0:\n",
    "\n",
    "            # Compute the network outputs at the training points.\n",
    "            N_u = model_u(x)\n",
    "\n",
    "            # Compute the trial solutions.\n",
    "            u = Y_trial_u(x, N_u)\n",
    "\n",
    "        # Compute the gradients of the trial solutions wrt inputs.\n",
    "        du_dx = tape0.gradient(u, x)\n",
    "\n",
    "        # Compute the estimates of the differential equations.\n",
    "        G_u = ode_u(x, u, du_dx)\n",
    "\n",
    "        # Compute the loss functions.\n",
    "        L_u = tf.math.sqrt(tf.reduce_sum(G_u**2)/n_train)\n",
    "        L = L_u\n",
    "\n",
    "    # Save the current losses.\n",
    "    losses_u.append(L_u)\n",
    "    losses.append(L.numpy())\n",
    "\n",
    "    # Check for convergence.\n",
    "    if epoch > 0:\n",
    "        loss_delta = losses[-1] - losses[-2]\n",
    "        if abs(loss_delta) <= tol:\n",
    "            converged = True\n",
    "            break\n",
    "\n",
    "    # Compute the gradient of the loss function wrt the network parameters.\n",
    "    pgrad_u = tape1.gradient(L, model_u.trainable_variables)\n",
    "\n",
    "    # Save the parameters used in this epoch.\n",
    "    phist_u.append(\n",
    "        np.hstack(\n",
    "            (model_u.trainable_variables[0].numpy().reshape((m*H,)),    # w (m, H) matrix -> (m*H,) row vector\n",
    "             model_u.trainable_variables[1].numpy(),       # u (H,) row vector\n",
    "             model_u.trainable_variables[2][:, 0].numpy()) # v (H, 1) column vector\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Update the parameters for this epoch.\n",
    "    optimizer.apply_gradients(zip(pgrad_u, model_u.trainable_variables))\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Ending epoch %s, loss function = %f\" % (epoch, L.numpy()))\n",
    "\n",
    "# Save the parameters used in the last epoch.\n",
    "phist_u.append(\n",
    "    np.hstack(\n",
    "        (model_u.trainable_variables[0].numpy().reshape((m*H,)),    # w (m, H) matrix -> (m*H,) row vector\n",
    "         model_u.trainable_variables[1].numpy(),       # u (H,) row vector\n",
    "         model_u.trainable_variables[2][:, 0].numpy()) # v (H, 1) column vector\n",
    "    )\n",
    ")\n",
    "\n",
    "n_epochs = epoch + 1\n",
    "\n",
    "t_stop = datetime.datetime.now()\n",
    "print(\"Training stopped at\", t_stop)\n",
    "t_elapsed = t_stop - t_start\n",
    "print(\"Total training time was %s seconds.\" % t_elapsed.total_seconds())\n",
    "print(\"Epochs: %d\" % n_epochs)\n",
    "print(\"Final value of loss function: %f\" % losses[-1])\n",
    "print(\"converged = %s\" % converged)\n",
    "\n",
    "# Save the parameter and loss function histories.\n",
    "np.savetxt(os.path.join(output_dir, 'phist_u.dat'), np.array(phist_u))\n",
    "np.savetxt(os.path.join(output_dir, 'losses.dat'), np.array(losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60db2549",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c58b2f-0cfa-458d-b52c-68cd600404c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and save the trained results at training points.\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    N_u = model_u(x)\n",
    "    ut_train = Y_trial_u(x, N_u)\n",
    "dut_dx_train = tape.gradient(ut_train, x)\n",
    "np.savetxt(os.path.join(output_dir, \"ut_train.dat\"), ut_train.numpy().reshape((n_train,)))\n",
    "np.savetxt(os.path.join(output_dir, \"dut_dx_train.dat\"), dut_dx_train.numpy())\n",
    "\n",
    "# Compute and save the analytical solution and derivative at training points.\n",
    "ua_train = u_analytical(x_train)\n",
    "dua_dx_train = du_dx_analytical(x_train)\n",
    "np.savetxt(os.path.join(output_dir,\"ua_train.dat\"), ua_train)\n",
    "np.savetxt(os.path.join(output_dir,\"dua_dx_train.dat\"), dua_dx_train)\n",
    "\n",
    "# Compute and save the error in the trained solution and derivative at training points.\n",
    "ut_err_train = ut_train.numpy().reshape((nx_train,)) - ua_train\n",
    "dut_dx_err_train = dut_dx_train.numpy().reshape((nx_train,)) - dua_dx_train\n",
    "np.savetxt(os.path.join(output_dir, \"ut_err_train.dat\"), ut_err_train)\n",
    "np.savetxt(os.path.join(output_dir, \"dut_dx_err_train.dat\"), dut_dx_err_train)\n",
    "\n",
    "# Compute the final RMS error in the solution at the training points.\n",
    "ut_rmse_train = np.sqrt(np.sum(ut_err_train**2)/n_train)\n",
    "print(\"ut_rmse_train = %s\" % ut_rmse_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ea1aba-cb1f-4de5-ad99-ca28fc447258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss function history.\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.semilogy(losses)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss function (RMS error)\")\n",
    "ax.grid()\n",
    "ax.set_title(\"Loss function evolution for %s\" % eq_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f64612-4548-4c69-a7b8-193ea7b5d161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the the trained solution and derivative at the training points.\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x_train, ut_train, label=\"$u_t$\")\n",
    "ax.plot(x_train, dut_dx_train, label=\"$du_t/dx$\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"$u_t$ or $du_t/dx$\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "ax.set_title(\"Trained solution and derivative for %s\" % eq_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5fc2c4-1298-4458-8b0a-68ce67d325c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the errors in the trained solution and derivative at the training points.\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x_train, ut_err_train, label=\"$u_t - u_a$\")\n",
    "ax.plot(x_train, dut_dx_err_train, label=\"$du_t/dx - du_a/dx$\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"Trained - analytical\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "ax.set_title(\"Error in trained solution for %s\" % eq_name)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
