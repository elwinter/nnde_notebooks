{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5169f6-efe8-487c-88a1-9f20541282d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard Python modules.\n",
    "import datetime\n",
    "import importlib\n",
    "import os\n",
    "import platform\n",
    "import sys\n",
    "\n",
    "# Import 3rd-party modules.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "# Import TensorFlow.\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35241708-a05f-4642-8bf2-793fb5c6a811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 64-bit math in TensorFlow.\n",
    "tf.keras.backend.set_floatx(\"float64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c38fc1-e90e-45ed-83e5-e2fd33d221d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Case 4: Compressible, isothermal, $\\frac {dP} {dx}$ variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f02e8a-6443-4ed3-8c3c-0e4884dbaf4a",
   "metadata": {},
   "source": [
    "Consider one-dimensional, isothermal, compressible fluid flow in a channel. For simplicity, assume the channel is a pipe with a circular cross-section of diameter $D$. Flow in the channel is controlled by the Bernoulli equation:\n",
    "\n",
    "\\begin{equation}\n",
    "    P + \\frac {1} {2} \\rho u^2 + \\rho g h = constant = C_1\n",
    "\\end{equation}\n",
    "\n",
    "where $P$ is the pressure, $\\rho$ is the mass density, $u$ is the fluid flow speed, $g$ is the acceleration due to gravity, and $h$ is the height of the fluid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1f3627-dba7-4d09-9489-f0a590353fcf",
   "metadata": {},
   "source": [
    "Neglecting gravity, this equation can be rearranged to solve for the flow speed $u$ as a function of the pressure $P$:\n",
    "\n",
    "\\begin{equation}\n",
    "    u = \\left[ \\frac {2} {\\rho} \\left( C_1 - P \\right) \\right]^{\\frac {1} {2}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6596f419-79e3-4b6e-8cf8-ab6daaf7940d",
   "metadata": {},
   "source": [
    "The pressure and density are related by the ideal gas law:\n",
    "\n",
    "\\begin{equation}\n",
    "    P = n k T = \\frac {\\rho} {m} k T\n",
    "\\end{equation}\n",
    "\n",
    "where $n$ is the number density, $m$ is the mass of a single fluid particle, $k$ is the Boltzmann constant, and $T$ is the temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9a305e-d21a-4db1-9bbd-d7b366b7dbeb",
   "metadata": {},
   "source": [
    "Rewriting the speed equation with this relation gives:\n",
    "\n",
    "\\begin{equation}\n",
    "    u = \\left[ \\frac {2 k T} {m P} \\left( C_1- P \\right) \\right]^{\\frac {1} {2}} \\\\\n",
    "    = C_3 \\left( \\frac {C_1} {P} - 1 \\right)^{\\frac {1} {2}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688329ac-7f10-4a11-9b96-c70b464e76a9",
   "metadata": {},
   "source": [
    "where the constant $C_3$ is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "    C_3 = \\left( \\frac {2 k T} {m} \\right)^{\\frac {1} {2}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32bde79-15b2-4226-ae5b-2692825a20fd",
   "metadata": {},
   "source": [
    "The change in speed is computed using the chain rule:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac {du} {dx} = \\frac {du} {dP} \\frac {dP} {dx} \\\\\n",
    "    = C_3 \\frac {1} {2} \\left( \\frac {C_1} {P} - 1 \\right)^{- \\frac {1} {2}} \\left( - \\frac {C_1} {P^2} \\right) \\frac {dP} {dx} \\\\\n",
    "    = - \\frac {C_1 C_3} {2} \\left[ P^3 \\left( C_1 - P \\right) \\right] ^{- \\frac {1} {2}} \\frac {dP} {dx}\n",
    "\\end{equation}\n",
    "\n",
    "where $x$ is the linear distance along the channel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca441c73-b117-402a-a25c-1a527e0a7029",
   "metadata": {},
   "source": [
    "Now assume the the pressure gradient $dP/dx$ is a function of the speed $u(x)$. The functional form uses the Darcy friction factor $f_D$:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac {dP} {dx} = - \\frac {f_D \\rho} {2 D} u^2\n",
    "\\end{equation}\n",
    "\n",
    "where $D$ is the pipe diameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c958fe-4484-40e2-8921-c82dad24a52d",
   "metadata": {},
   "source": [
    "We now have a coupled set of ordinary differential equations for $u(x)$ and $P(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79afc84-8aa6-44cd-afcc-3e0f27cb50ca",
   "metadata": {},
   "source": [
    "Now assume $\\rho_0 = 1$, $P_0 = 1$, $u_0 = 1$, and $\\frac {f_D} {D} = 1$, and select $m$ and $T$ such that $C_3 = 1$. The constant $C_1 = \\frac {3} {2}$, and the differential equations become:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac {du} {dx} = - \\frac {3} {4} \\left[ P^3 \\left( \\frac {3} {2} - P \\right) \\right] ^{-1/2} \\frac {dP} {dx} \\\\\n",
    "    \\frac {dP} {dx} = - \\frac {1} {2} u^2\n",
    "\\end{equation}\n",
    "\n",
    "This system of ODEs can be solved numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed0dc5a-075b-4ea7-9bc7-c9cbfcead037",
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_name = \"case4\"\n",
    "\n",
    "# Define the differential equations to be numerically integrated.\n",
    "def dY_dt(Y, x):\n",
    "    (u, P) = Y\n",
    "    dP_dx = -0.5*P*u**2\n",
    "    du_dx = -3/2*(P**3*(3 - 2*P))**(-0.5)*dP_dx\n",
    "    return [du_dx, dP_dx]\n",
    "\n",
    "# Compute the numerical solutions.\n",
    "nx = 10001\n",
    "xn = np.linspace(0, 1, nx)\n",
    "Y0 = [1, 1]  # Initial conditions\n",
    "Y = odeint(dY_dt, Y0, xn)\n",
    "un = Y[:, 0]\n",
    "Pn = Y[:, 1]\n",
    "\n",
    "# Compute the derivatives of the numerical solutions.\n",
    "dun = np.gradient(un)\n",
    "dPn = np.gradient(Pn)\n",
    "dxn = np.gradient(xn)\n",
    "dun_dx = dun/dxn\n",
    "dPn_dx = dPn/dxn\n",
    "\n",
    "# Plot the numerical solutions and derivatives.\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(xn, un, label=\"$u$\")\n",
    "ax.plot(xn, Pn, label=\"$P$\")\n",
    "ax.plot(xn, dun_dx, label=\"$du/dx$\")\n",
    "ax.plot(xn, dPn_dx, label=\"$dP/dx$\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"$u$, $P$, $du/dx$, or $dP/dx$\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "ax.set_title(\"Numerical solutions and derivatives for %s\" % eq_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f775fe50-2915-45a7-be0f-9626078ea930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_system_information():\n",
    "    print(\"System report:\")\n",
    "    print(datetime.datetime.now())\n",
    "    print(\"Host name: %s\" % platform.node())\n",
    "    print(\"OS: %s\" % platform.platform())\n",
    "    print(\"uname:\", platform.uname())\n",
    "    print(\"Python version: %s\" % sys.version)\n",
    "    print(\"Python build:\", platform.python_build())\n",
    "    print(\"Python compiler: %s\" % platform.python_compiler())\n",
    "    print(\"Python implementation: %s\" % platform.python_implementation())\n",
    "    # print(\"Python file: %s\" % __file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5108993-c34b-4665-8b34-eca840a2710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_output_directory(path=None):\n",
    "    path_noext, ext = os.path.splitext(path)\n",
    "    output_dir = path_noext\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "    return output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a7d07f-be4d-4c51-8fcb-d9ad048c88cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnde.math.trainingdata import create_training_grid2\n",
    "\n",
    "def create_training_data(*n_train):\n",
    "    x_train = np.linspace(0, 0.9, n_train[0])\n",
    "    return x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7a172e-2e3b-436e-a8c2-30ec3a39686b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(H, w0_range, u0_range, v0_range):\n",
    "    hidden_layer_1 = tf.keras.layers.Dense(\n",
    "        units=H, use_bias=True,\n",
    "        activation=tf.keras.activations.sigmoid,\n",
    "        kernel_initializer=tf.keras.initializers.RandomUniform(*w0_range),\n",
    "        bias_initializer=tf.keras.initializers.RandomUniform(*u0_range)\n",
    "    )\n",
    "    output_layer = tf.keras.layers.Dense(\n",
    "        units=1,\n",
    "        activation=tf.keras.activations.linear,\n",
    "        kernel_initializer=tf.keras.initializers.RandomUniform(*v0_range),\n",
    "        use_bias=False,\n",
    "    )\n",
    "    model = tf.keras.Sequential([hidden_layer_1, output_layer])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c85ace-8b94-4371-a963-172448629e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_system_information()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e603488d-9f36-4c11-9788-4b13bcf1b83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the output directory.\n",
    "eq_name = \"case4\"\n",
    "path = os.path.join(\".\", eq_name)\n",
    "output_dir = create_output_directory(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220421d2-169f-48ae-a3c7-ec5bc4e2ac6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters.\n",
    "\n",
    "# Training optimizer\n",
    "optimizer_name = \"Adam\"\n",
    "\n",
    "# Initial parameter ranges\n",
    "w0_range = [-0.1, 0.1]\n",
    "u0_range = [-0.1, 0.1]\n",
    "v0_range = [-0.1, 0.1]\n",
    "\n",
    "# Maximum number of training epochs.\n",
    "max_epochs = 40000\n",
    "\n",
    "# Learning rate.\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Absolute tolerance for consecutive loss function values to indicate convergence.\n",
    "tol = 1e-6\n",
    "\n",
    "# Number of hidden nodes.\n",
    "H = 10\n",
    "\n",
    "# Number of dimensions\n",
    "m = 1\n",
    "\n",
    "# Number of training points in each dimension.\n",
    "nx_train = 11\n",
    "n_train = nx_train\n",
    "\n",
    "# Random number generator seed.\n",
    "random_seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa423e4b-b35c-4e83-ab82-412d8d83d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save the training data.\n",
    "x_train = create_training_data(nx_train)\n",
    "np.savetxt(os.path.join(output_dir,'x_train.dat'), x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8621e73c-c912-470b-a22a-808b28007903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the differential equations using TensorFlow operations.\n",
    "\n",
    "@tf.function\n",
    "def ode_u(x, u, P, du_dx, dP_dx):\n",
    "    G = du_dx + 3/2*(P**3*(3 - 2*P))**(-0.5)*dP_dx\n",
    "    return G\n",
    "\n",
    "@tf.function\n",
    "def ode_P(x, u, P, du_dx, dP_dx):\n",
    "    G = dP_dx + 0.5*P*u**2\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4979edb5-19bc-4e1e-87e9-4691e3ac1c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the trial functions.\n",
    "\n",
    "@tf.function\n",
    "def Y_trial_u(x, N):\n",
    "    A = tf.constant([[1.0]], dtype=\"float64\")\n",
    "    P = x\n",
    "    Y = A + P*N\n",
    "    return Y\n",
    "\n",
    "@tf.function\n",
    "def Y_trial_P(x, N):\n",
    "    A = tf.constant([[1.0]], dtype=\"float64\")\n",
    "    P = x\n",
    "    Y = A + P*N\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dd6c43-e136-4755-a99f-ad0357f2e444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the models.\n",
    "model_u = build_model(H, w0_range, u0_range, v0_range)\n",
    "model_P = build_model(H, w0_range, u0_range, v0_range)\n",
    "\n",
    "# Create the optimizer.\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Create history variables.\n",
    "losses_u = []\n",
    "losses_P = []\n",
    "losses = []\n",
    "phist_u = []\n",
    "phist_P = []\n",
    "\n",
    "# Set the random number seed for reproducibility.\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "# Rename the training data Variable for convenience, just for training.\n",
    "# shape (n_train, m)\n",
    "x_train_var = tf.Variable(np.array(x_train).reshape((n_train, m)), name=\"x_train\")\n",
    "x = x_train_var\n",
    "\n",
    "# Clear the convergence flag to start.\n",
    "converged = False\n",
    "\n",
    "# Train the model.\n",
    "\n",
    "print(\"Hyperparameters: n_train = %s, H = %s, max_epochs = %s, optimizer = %s, learning_rate = %s\"\n",
    "      % (n_train, H, max_epochs, optimizer_name, learning_rate))\n",
    "t_start = datetime.datetime.now()\n",
    "print(\"Training started at\", t_start)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "\n",
    "    # Run the forward pass.\n",
    "    with tf.GradientTape(persistent=True) as tape1:\n",
    "        with tf.GradientTape(persistent=True) as tape0:\n",
    "\n",
    "            # Compute the network outputs at the training points.\n",
    "            N_u = model_u(x)\n",
    "            N_P = model_P(x)\n",
    "\n",
    "            # Compute the trial solutions.\n",
    "            u = Y_trial_u(x, N_u)\n",
    "            P = Y_trial_P(x, N_P)\n",
    "\n",
    "        # Compute the gradients of the trial solutions wrt inputs.\n",
    "        du_dx = tape0.gradient(u, x)\n",
    "        dP_dx = tape0.gradient(P, x)\n",
    "\n",
    "        # Compute the estimates of the differential equations.\n",
    "        G_u = ode_u(x, u, P, du_dx, dP_dx)\n",
    "        G_P = ode_P(x, u, P, du_dx, dP_dx)\n",
    "\n",
    "        # Compute the loss functions.\n",
    "        L_u = tf.math.sqrt(tf.reduce_sum(G_u**2)/n_train)\n",
    "        L_P = tf.math.sqrt(tf.reduce_sum(G_P**2)/n_train)\n",
    "        L = L_u + L_P\n",
    "\n",
    "    # Save the current losses.\n",
    "    losses_u.append(L_u)\n",
    "    losses_P.append(L_P)\n",
    "    losses.append(L.numpy())\n",
    "\n",
    "    # Check for convergence.\n",
    "    if epoch > 0:\n",
    "        loss_delta = losses[-1] - losses[-2]\n",
    "        if abs(loss_delta) <= tol:\n",
    "            converged = True\n",
    "            break\n",
    "\n",
    "    # Compute the gradient of the loss function wrt the network parameters.\n",
    "    pgrad_u = tape1.gradient(L, model_u.trainable_variables)\n",
    "    pgrad_P = tape1.gradient(L, model_P.trainable_variables)\n",
    "\n",
    "    # Save the parameters used in this epoch.\n",
    "    phist_u.append(\n",
    "        np.hstack(\n",
    "            (model_u.trainable_variables[0].numpy().reshape((m*H,)),    # w (m, H) matrix -> (m*H,) row vector\n",
    "             model_u.trainable_variables[1].numpy(),       # u (H,) row vector\n",
    "             model_u.trainable_variables[2][:, 0].numpy()) # v (H, 1) column vector\n",
    "        )\n",
    "    )\n",
    "    phist_P.append(\n",
    "        np.hstack(\n",
    "            (model_P.trainable_variables[0].numpy().reshape((m*H,)),    # w (m, H) matrix -> (m*H,) row vector\n",
    "             model_P.trainable_variables[1].numpy(),       # u (H,) row vector\n",
    "             model_P.trainable_variables[2][:, 0].numpy()) # v (H, 1) column vector\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Update the parameters for this epoch.\n",
    "    optimizer.apply_gradients(zip(pgrad_u, model_u.trainable_variables))\n",
    "    optimizer.apply_gradients(zip(pgrad_P, model_P.trainable_variables))\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Ending epoch %s, loss function = %f\" % (epoch, L.numpy()))\n",
    "\n",
    "# Save the parameters used in the last epoch.\n",
    "phist_u.append(\n",
    "    np.hstack(\n",
    "        (model_u.trainable_variables[0].numpy().reshape((m*H,)),    # w (m, H) matrix -> (m*H,) row vector\n",
    "         model_u.trainable_variables[1].numpy(),       # u (H,) row vector\n",
    "         model_u.trainable_variables[2][:, 0].numpy()) # v (H, 1) column vector\n",
    "    )\n",
    ")\n",
    "phist_P.append(\n",
    "    np.hstack(\n",
    "        (model_P.trainable_variables[0].numpy().reshape((m*H,)),    # w (m, H) matrix -> (m*H,) row vector\n",
    "         model_P.trainable_variables[1].numpy(),       # u (H,) row vector\n",
    "         model_P.trainable_variables[2][:, 0].numpy()) # v (H, 1) column vector\n",
    "    )\n",
    ")\n",
    "\n",
    "n_epochs = epoch + 1\n",
    "\n",
    "t_stop = datetime.datetime.now()\n",
    "print(\"Training stopped at\", t_stop)\n",
    "t_elapsed = t_stop - t_start\n",
    "print(\"Total training time was %s seconds.\" % t_elapsed.total_seconds())\n",
    "print(\"Epochs: %d\" % n_epochs)\n",
    "print(\"Final value of loss function: %f\" % losses[-1])\n",
    "print(\"converged = %s\" % converged)\n",
    "\n",
    "# Save the parameter and loss function histories.\n",
    "np.savetxt(os.path.join(output_dir, 'phist_u.dat'), np.array(phist_u))\n",
    "np.savetxt(os.path.join(output_dir, 'phist_P.dat'), np.array(phist_P))\n",
    "np.savetxt(os.path.join(output_dir, 'losses.dat'), np.array(losses))\n",
    "np.savetxt(os.path.join(output_dir, 'losses_u.dat'), np.array(losses_u))\n",
    "np.savetxt(os.path.join(output_dir, 'losses_P.dat'), np.array(losses_P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c58b2f-0cfa-458d-b52c-68cd600404c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and save the trained results at training points.\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    N_u = model_u(x)\n",
    "    N_P = model_P(x)\n",
    "    ut_train = Y_trial_u(x, N_u)\n",
    "    Pt_train = Y_trial_P(x, N_P)\n",
    "dut_dx_train = tape.gradient(ut_train, x)\n",
    "dPt_dx_train = tape.gradient(Pt_train, x)\n",
    "np.savetxt(os.path.join(output_dir, 'ut_train.dat'), ut_train.numpy().reshape((n_train,)))\n",
    "np.savetxt(os.path.join(output_dir, 'Pt_train.dat'), Pt_train.numpy().reshape((n_train,)))\n",
    "np.savetxt(os.path.join(output_dir, 'dut_dx_train.dat'), dut_dx_train.numpy())\n",
    "np.savetxt(os.path.join(output_dir, 'dPt_dx_train.dat'), dPt_dx_train.numpy())\n",
    "\n",
    "# Compute and save the numerical solutions and derivatives at training points.\n",
    "un_train = np.interp(x_train, xn, un)\n",
    "Pn_train = np.interp(x_train, xn, Pn)\n",
    "dun_dx_train = np.interp(x_train, xn, dun_dx)\n",
    "dPn_dx_train = np.interp(x_train, xn, dPn_dx)\n",
    "np.savetxt(os.path.join(output_dir, 'un_train.dat'), un_train)\n",
    "np.savetxt(os.path.join(output_dir, 'Pn_train.dat'), Pn_train)\n",
    "np.savetxt(os.path.join(output_dir, 'dun_dx_train.dat'), dun_dx_train)\n",
    "np.savetxt(os.path.join(output_dir, 'dPn_dx_train.dat'), dPn_dx_train)\n",
    "\n",
    "# Compute and save the error in the trained solutions and derivatives at training points.\n",
    "ut_err_train = ut_train.numpy().reshape((nx_train,)) - un_train\n",
    "Pt_err_train = Pt_train.numpy().reshape((nx_train,)) - Pn_train\n",
    "dut_dx_err_train = dut_dx_train.numpy().reshape((nx_train,)) - dun_dx_train\n",
    "dPt_dx_err_train = dPt_dx_train.numpy().reshape((nx_train,)) - dPn_dx_train\n",
    "np.savetxt(os.path.join(output_dir, 'ut_err_train.dat'), ut_err_train)\n",
    "np.savetxt(os.path.join(output_dir, 'Pt_err_train.dat'), Pt_err_train)\n",
    "np.savetxt(os.path.join(output_dir, 'dut_dx_err_train.dat'), dut_dx_err_train)\n",
    "np.savetxt(os.path.join(output_dir, 'dPt_dx_err_train.dat'), dPt_dx_err_train)\n",
    "\n",
    "# Compute the final RMS errors in the solutions at the training points.\n",
    "ut_rmse_train = np.sqrt(np.sum(ut_err_train**2)/n_train)\n",
    "Pt_rmse_train = np.sqrt(np.sum(Pt_err_train**2)/n_train)\n",
    "print(\"ut_rmse_train = %s\" % ut_rmse_train)\n",
    "print(\"Pt_rmse_train = %s\" % Pt_rmse_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ea1aba-cb1f-4de5-ad99-ca28fc447258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss function histories.\n",
    "plt.semilogy(losses, label=\"L (total)\")\n",
    "plt.semilogy(losses_u, label=\"L (u)\")\n",
    "plt.semilogy(losses_P, label=\"L (P)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss function\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Loss function evolution for %s\\n%s optimizer, $\\eta$=%s, H=%s, $n_{train}$=%s, epochs=%s\" %\n",
    "          (eq_name, optimizer_name, learning_rate, H, n_train, n_epochs))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5fc2c4-1298-4458-8b0a-68ce67d325c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the the trained solutions and derivatives at the training points.\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x_train, ut_train, label=\"$u_t$\")\n",
    "ax.plot(x_train, Pt_train, label=\"$P_t$\")\n",
    "ax.plot(x_train, dut_dx_train, label=\"$du_t/dx$\")\n",
    "ax.plot(x_train, dPt_dx_train, label=\"$dP_t/dx$\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"$u_t$ or $du_t/dx$\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "ax.set_title(\"Trained solutions and derivatives for %s\" % eq_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65cd4c4-437a-4990-b98a-c98119cfccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the errors in the trained solutions and derivatives at the training points.\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x_train, ut_err_train, label=\"$u_t - u_n$\")\n",
    "ax.plot(x_train, Pt_err_train, label=\"$P_t - P_n$\")\n",
    "ax.plot(x_train, dut_dx_err_train, label=\"$du_t/dx - du_a/dx$\")\n",
    "ax.plot(x_train, dPt_dx_err_train, label=\"$dP_t/dx - dP_a/dx$\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"Trained - numerical$\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "ax.set_title(\"Error in trained solution for %s\" % eq_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dacdb6-6972-4d93-a9ae-b9a1b521755d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
